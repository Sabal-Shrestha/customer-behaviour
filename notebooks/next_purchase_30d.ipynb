{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "29a3e5b6",
   "metadata": {},
   "source": [
    "# Analysing customer behaviour: Next Purchase in 30 Days\n",
    "This notebook sets up a reproducible workflow to predict whether a customer will make a purchase within the next 30 days using the existing `data/transaction-dataset.csv`. It includes environment setup, data ingestion, label generation, simple RFM-style features, a Logistic Regression baseline, evaluation, and outputs.\n",
    "\n",
    "> Plain-English naming used throughout:\n",
    "> - snapshot_date: the reference date for a customer (their most recent transaction inside a split window). This was previously called anchor_date.\n",
    "> - label_next_30d: 1 if the customer purchases within 30 days after the snapshot_date, else 0.\n",
    "> - predicted_probability_30d: the model’s estimated probability that a customer will purchase within 30 days.\n",
    "> - is_top_10_percent: whether the customer falls in the top 10% by predicted_probability_30d for a given split.\n",
    "> - split windows: train/val/test time ranges built from the transaction dates, using only “safe” snapshot dates that are at least 30 days before the dataset’s max date.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d314a0a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: /Users/primeiscrime/Desktop/customer\n",
      "Running in VS Code: True\n"
     ]
    }
   ],
   "source": [
    "# 1) Set Up Environment and Paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "# Heuristic project root detection: prefer parent of notebooks/ if data/ exists there\n",
    "_nb_cwd = Path.cwd().resolve()\n",
    "_candidate_root = _nb_cwd if (_nb_cwd / \"data\").exists() else _nb_cwd.parent\n",
    "project_root = Path(os.getenv(\"PROJECT_ROOT\", str(_candidate_root))).resolve()\n",
    "\n",
    "data_dir = project_root / \"data\"\n",
    "output_dir = project_root / \"outputs\"\n",
    "logs_dir = project_root / \"logs\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "logs_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Detect if running inside VS Code Jupyter\n",
    "in_vscode = bool(os.getenv(\"VSCODE_PID\"))\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Running in VS Code: {in_vscode}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "510e7553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "{'numpy': '2.0.2', 'pandas': '2.3.3', 'matplotlib': '3.9.4', 'seaborn': '0.13.2', 'scikit_learn': '1.6.1'}\n",
      "{'numpy': '2.0.2', 'pandas': '2.3.3', 'matplotlib': '3.9.4', 'seaborn': '0.13.2', 'scikit_learn': '1.6.1'}\n"
     ]
    }
   ],
   "source": [
    "# 2) Install/Verify Dependencies\n",
    "%pip install -q --disable-pip-version-check --no-input --upgrade numpy pandas matplotlib seaborn scikit-learn pytest pyarrow || true\n",
    "\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "import matplotlib as _mpl\n",
    "import seaborn as _sns\n",
    "import sklearn as _sk\n",
    "\n",
    "print({\n",
    "    \"numpy\": _np.__version__,\n",
    "    \"pandas\": _pd.__version__,\n",
    "    \"matplotlib\": _mpl.__version__,\n",
    "    \"seaborn\": _sns.__version__,\n",
    "    \"scikit_learn\": _sk.__version__,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb527450",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Imports and Global Config\n",
    "import sys\n",
    "import json\n",
    "import logging\n",
    "from typing import Tuple, Dict, Any, Optional\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# Reproducibility\n",
    "RANDOM_SEED = int(os.getenv(\"SEED\", 42))\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Pandas display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)\n",
    "sns.set_theme(style=\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2cc41e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:30:57,183 | INFO | Python 3.9.6 (default, Aug  8 2025, 19:06:38) \n",
      "[Clang 17.0.0 (clang-1700.3.19.1)]\n",
      "2025-10-25 23:30:57,184 | INFO | Project root: /Users/primeiscrime/Desktop/customer\n",
      "2025-10-25 23:30:57,185 | INFO | Outputs: /Users/primeiscrime/Desktop/customer/outputs\n",
      "2025-10-25 23:30:57,184 | INFO | Project root: /Users/primeiscrime/Desktop/customer\n",
      "2025-10-25 23:30:57,185 | INFO | Outputs: /Users/primeiscrime/Desktop/customer/outputs\n"
     ]
    }
   ],
   "source": [
    "# 4) Logging Configuration\n",
    "log_level = os.getenv(\"LOGLEVEL\", \"INFO\").upper()\n",
    "logger = logging.getLogger(\"customer_behaviour\")\n",
    "logger.setLevel(log_level)\n",
    "formatter = logging.Formatter(\"%(asctime)s | %(levelname)s | %(message)s\")\n",
    "\n",
    "# Console handler\n",
    "ch = logging.StreamHandler(sys.stdout)\n",
    "ch.setLevel(log_level)\n",
    "ch.setFormatter(formatter)\n",
    "logger.addHandler(ch)\n",
    "\n",
    "# File handler\n",
    "fh = logging.FileHandler(logs_dir / \"notebook.log\")\n",
    "fh.setLevel(log_level)\n",
    "fh.setFormatter(formatter)\n",
    "logger.addHandler(fh)\n",
    "\n",
    "logger.info(f\"Python {sys.version}\")\n",
    "logger.info(f\"Project root: {project_root}\")\n",
    "logger.info(f\"Outputs: {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db5fe787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Data I/O Helpers\n",
    "from typing import List\n",
    "def load_csv(path: Path, **kwargs) -> pd.DataFrame:\n",
    "    try:\n",
    "        df = pd.read_csv(path, **kwargs)\n",
    "        if df.empty:\n",
    "            raise ValueError(f\"CSV at {path} loaded but is empty.\")\n",
    "        return df\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"CSV not found at {path}\") from e\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load CSV {path}: {e}\") from e\n",
    "\n",
    "def save_csv(df: pd.DataFrame, path: Path, index: bool = False) -> None:\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df.to_csv(path, index=index)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save CSV to {path}: {e}\") from e\n",
    "\n",
    "def load_json(path: Path) -> Dict[str, Any]:\n",
    "    try:\n",
    "        with open(path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"JSON not found at {path}\") from e\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to load JSON {path}: {e}\") from e\n",
    "\n",
    "def save_json(obj: Dict[str, Any], path: Path) -> None:\n",
    "    try:\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(path, \"w\") as f:\n",
    "            json.dump(obj, f, indent=2)\n",
    "    except Exception as e:\n",
    "        raise RuntimeError(f\"Failed to save JSON to {path}: {e}\") from e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "141f7a67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:31:20,326 | INFO | Loading CSV from /Users/primeiscrime/Desktop/customer/data/transaction-dataset.csv\n",
      "2025-10-25 23:31:20,456 | INFO | Loaded shape: (200000, 10)\n",
      "2025-10-25 23:31:20,469 | INFO | Date range: 2023-01-01 to 2023-11-26\n",
      "2025-10-25 23:31:20,456 | INFO | Loaded shape: (200000, 10)\n",
      "2025-10-25 23:31:20,469 | INFO | Date range: 2023-01-01 to 2023-11-26\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>ProductID</th>\n",
       "      <th>Quantity</th>\n",
       "      <th>Price</th>\n",
       "      <th>TransactionDate</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>StoreLocation</th>\n",
       "      <th>ProductCategory</th>\n",
       "      <th>DiscountApplied(%)</th>\n",
       "      <th>TotalAmount</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>C000047</td>\n",
       "      <td>P000474</td>\n",
       "      <td>1</td>\n",
       "      <td>39.05</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>21.07</td>\n",
       "      <td>30.83</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>C000116</td>\n",
       "      <td>P000332</td>\n",
       "      <td>1</td>\n",
       "      <td>21.07</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Books</td>\n",
       "      <td>6.25</td>\n",
       "      <td>19.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>C000116</td>\n",
       "      <td>P000019</td>\n",
       "      <td>1</td>\n",
       "      <td>50.52</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Miami</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>0.00</td>\n",
       "      <td>50.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>C000227</td>\n",
       "      <td>P000379</td>\n",
       "      <td>2</td>\n",
       "      <td>33.04</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Cash</td>\n",
       "      <td>Phoenix</td>\n",
       "      <td>Beauty</td>\n",
       "      <td>6.82</td>\n",
       "      <td>61.58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>C000343</td>\n",
       "      <td>P000027</td>\n",
       "      <td>4</td>\n",
       "      <td>108.60</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>Credit Card</td>\n",
       "      <td>Seattle</td>\n",
       "      <td>Home &amp; Kitchen</td>\n",
       "      <td>24.54</td>\n",
       "      <td>327.81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  CustomerID ProductID  Quantity   Price TransactionDate PaymentMethod StoreLocation ProductCategory  \\\n",
       "0    C000047   P000474         1   39.05      2023-01-01   Credit Card       Phoenix          Beauty   \n",
       "1    C000116   P000332         1   21.07      2023-01-01   Credit Card         Miami           Books   \n",
       "2    C000116   P000019         1   50.52      2023-01-01          Cash         Miami          Beauty   \n",
       "3    C000227   P000379         2   33.04      2023-01-01          Cash       Phoenix          Beauty   \n",
       "4    C000343   P000027         4  108.60      2023-01-01   Credit Card       Seattle  Home & Kitchen   \n",
       "\n",
       "   DiscountApplied(%)  TotalAmount  \n",
       "0               21.07        30.83  \n",
       "1                6.25        19.75  \n",
       "2                0.00        50.52  \n",
       "3                6.82        61.58  \n",
       "4               24.54       327.81  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Quantity</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>1.998450</td>\n",
       "      <td>1.069071</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2.00</td>\n",
       "      <td>3.00</td>\n",
       "      <td>4.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Price</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>64.834650</td>\n",
       "      <td>76.479172</td>\n",
       "      <td>2.74</td>\n",
       "      <td>17.72</td>\n",
       "      <td>39.60</td>\n",
       "      <td>78.59</td>\n",
       "      <td>455.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DiscountApplied(%)</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>4.905064</td>\n",
       "      <td>8.197170</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>7.54</td>\n",
       "      <td>30.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>TotalAmount</th>\n",
       "      <td>200000.0</td>\n",
       "      <td>123.078226</td>\n",
       "      <td>177.904795</td>\n",
       "      <td>2.17</td>\n",
       "      <td>26.73</td>\n",
       "      <td>62.91</td>\n",
       "      <td>139.92</td>\n",
       "      <td>1790.25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       count        mean         std   min    25%    50%     75%      max\n",
       "Quantity            200000.0    1.998450    1.069071  1.00   1.00   2.00    3.00     4.00\n",
       "Price               200000.0   64.834650   76.479172  2.74  17.72  39.60   78.59   455.54\n",
       "DiscountApplied(%)  200000.0    4.905064    8.197170  0.00   0.00   0.00    7.54    30.00\n",
       "TotalAmount         200000.0  123.078226  177.904795  2.17  26.73  62.91  139.92  1790.25"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>column</th>\n",
       "      <th>n_unique</th>\n",
       "      <th>top5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CustomerID</td>\n",
       "      <td>26360</td>\n",
       "      <td>C021143:147, C000720:144, C001928:112, C001429...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ProductID</td>\n",
       "      <td>600</td>\n",
       "      <td>P000213:383, P000470:381, P000512:379, P000090...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PaymentMethod</td>\n",
       "      <td>4</td>\n",
       "      <td>Credit Card:110230, Debit Card:39902, Digital ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>StoreLocation</td>\n",
       "      <td>8</td>\n",
       "      <td>Los Angeles:25250, Seattle:25125, Miami:25076,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ProductCategory</td>\n",
       "      <td>8</td>\n",
       "      <td>Clothing:28871, Home &amp; Kitchen:28232, Books:26...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            column  n_unique                                               top5\n",
       "0       CustomerID     26360  C021143:147, C000720:144, C001928:112, C001429...\n",
       "1        ProductID       600  P000213:383, P000470:381, P000512:379, P000090...\n",
       "2    PaymentMethod         4  Credit Card:110230, Debit Card:39902, Digital ...\n",
       "3    StoreLocation         8  Los Angeles:25250, Seattle:25125, Miami:25076,...\n",
       "4  ProductCategory         8  Clothing:28871, Home & Kitchen:28232, Books:26..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No missing values detected across columns.\n"
     ]
    }
   ],
   "source": [
    "# 6) Data Ingestion and Preview (clean summaries)\n",
    "CSV_PATH = Path(os.getenv(\"TX_CSV\", data_dir / \"transaction-dataset.csv\"))\n",
    "logger.info(f\"Loading CSV from {CSV_PATH}\")\n",
    "df = load_csv(CSV_PATH)\n",
    "logger.info(f\"Loaded shape: {df.shape}\")\n",
    "\n",
    "# Basic parsing and types\n",
    "if df['TransactionDate'].dtype != 'datetime64[ns]':\n",
    "    df['TransactionDate'] = pd.to_datetime(df['TransactionDate'], errors='coerce')\n",
    "missing_dates = int(df['TransactionDate'].isna().sum())\n",
    "if missing_dates > 0:\n",
    "    logger.warning(f\"Rows with unparsable TransactionDate: {missing_dates}\")\n",
    "    df = df.dropna(subset=['TransactionDate']).copy()\n",
    "\n",
    "# Column presence check\n",
    "expected_cols = [\n",
    "    'CustomerID','ProductID','Quantity','Price','TransactionDate',\n",
    "    'PaymentMethod','StoreLocation','ProductCategory','DiscountApplied(%)','TotalAmount'\n",
    "]\n",
    "missing_cols = [c for c in expected_cols if c not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"Missing expected columns: {missing_cols}\")\n",
    "\n",
    "min_date = df['TransactionDate'].min()\n",
    "max_date = df['TransactionDate'].max()\n",
    "logger.info(f\"Date range: {min_date.date()} to {max_date.date()}\")\n",
    "\n",
    "display(df.head())\n",
    "\n",
    "# Clean numeric summary (avoids NaNs for non-numeric fields)\n",
    "numeric_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "if numeric_cols:\n",
    "    display(df[numeric_cols].describe(percentiles=[.25,.5,.75]).T)\n",
    "\n",
    "# Clean categorical summary: n_unique and top-5 values per column\n",
    "cat_cols = df.select_dtypes(include=['object','category']).columns.tolist()\n",
    "if cat_cols:\n",
    "    cat_summary_rows = []\n",
    "    for c in cat_cols:\n",
    "        vc = df[c].value_counts(dropna=False)\n",
    "        top5 = \", \".join([f\"{idx}:{cnt}\" for idx, cnt in vc.head(5).items()])\n",
    "        cat_summary_rows.append({\n",
    "            'column': c,\n",
    "            'n_unique': df[c].nunique(dropna=True),\n",
    "            'top5': top5\n",
    "        })\n",
    "    display(pd.DataFrame(cat_summary_rows))\n",
    "\n",
    "# Missingness summary (only show columns with >0% missing)\n",
    "missing_pct = df.isna().mean().sort_values(ascending=False)\n",
    "missing_pct = missing_pct[missing_pct > 0]\n",
    "if not missing_pct.empty:\n",
    "    display(missing_pct)\n",
    "else:\n",
    "    print(\"No missing values detected across columns.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48d1cc62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-10-25 23:31:24,701 | INFO | Split windows: train>=2023-01-01 val>=2023-06-29 test>=2023-08-28 last_safe_snapshot=2023-10-27\n",
      "2025-10-25 23:31:24,794 | INFO | Snapshots: train=23962 val=16693 test=16794\n",
      "2025-10-25 23:31:24,794 | INFO | Snapshots: train=23962 val=16693 test=16794\n",
      "2025-10-25 23:31:56,220 | INFO | Positives (train/val/test): 0.175 / 0.253 / 0.253\n",
      "2025-10-25 23:31:56,220 | INFO | Positives (train/val/test): 0.175 / 0.253 / 0.253\n",
      "2025-10-25 23:33:10,885 | INFO | Feature shapes: X_train=(23962, 20), X_val=(16693, 20), X_test=(16794, 20)\n",
      "2025-10-25 23:33:10,885 | INFO | Feature shapes: X_train=(23962, 20), X_val=(16693, 20), X_test=(16794, 20)\n"
     ]
    }
   ],
   "source": [
    "# 7) Core Computation: Splits, Labels, and Features\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "LABEL_HORIZON_DAYS = 30\n",
    "FEATURE_WINDOW_DAYS = 90\n",
    "\n",
    "@dataclass\n",
    "class SplitWindows:\n",
    "    train_start: pd.Timestamp\n",
    "    val_start: pd.Timestamp\n",
    "    test_start: pd.Timestamp\n",
    "    last_safe_snapshot: pd.Timestamp  # last date we can safely take a snapshot (max_date - label_horizon)\n",
    "\n",
    "def compute_split_windows(df: pd.DataFrame, label_horizon_days: int = 30) -> SplitWindows:\n",
    "    min_d = df['TransactionDate'].min().normalize()\n",
    "    max_d = df['TransactionDate'].max().normalize()\n",
    "    last_safe_snapshot = max_d - pd.Timedelta(days=label_horizon_days)\n",
    "    # Proportional cut points at 60% and 80% of the safe window\n",
    "    safe_days = (last_safe_snapshot - min_d).days\n",
    "    cut1 = min_d + pd.Timedelta(days=int(safe_days * 0.6))\n",
    "    cut2 = min_d + pd.Timedelta(days=int(safe_days * 0.8))\n",
    "    return SplitWindows(train_start=min_d, val_start=cut1, test_start=cut2, last_safe_snapshot=last_safe_snapshot)\n",
    "\n",
    "splits = compute_split_windows(df, LABEL_HORIZON_DAYS)\n",
    "logger.info(f\"Split windows: train>={splits.train_start.date()} val>={splits.val_start.date()} test>={splits.test_start.date()} last_safe_snapshot={splits.last_safe_snapshot.date()}\")\n",
    "\n",
    "def filter_by_window(df: pd.DataFrame, start: pd.Timestamp, end: pd.Timestamp) -> pd.DataFrame:\n",
    "    m = (df['TransactionDate'] >= start) & (df['TransactionDate'] <= end)\n",
    "    return df.loc[m].copy()\n",
    "\n",
    "def last_snapshot_per_customer(df_window: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Pick the last transaction per customer within the window as their snapshot (reference) date.\"\"\"\n",
    "    idx = df_window.sort_values('TransactionDate').groupby('CustomerID')['TransactionDate'].idxmax()\n",
    "    snapshots = df_window.loc[idx].copy()\n",
    "    snapshots = snapshots[['CustomerID', 'TransactionDate']]\n",
    "    snapshots = snapshots.rename(columns={'TransactionDate': 'snapshot_date'})\n",
    "    return snapshots.reset_index(drop=True)\n",
    "\n",
    "def has_purchase_within(df_cust: pd.DataFrame, start_exclusive: pd.Timestamp, end_inclusive: pd.Timestamp) -> bool:\n",
    "    return ((df_cust['TransactionDate'] > start_exclusive) & (df_cust['TransactionDate'] <= end_inclusive)).any()\n",
    "\n",
    "def label_next_30d(df_all: pd.DataFrame, snapshots: pd.DataFrame, horizon_days: int = 30) -> pd.DataFrame:\n",
    "    labels = []\n",
    "    by_cust = {cid: grp.sort_values('TransactionDate') for cid, grp in df_all.groupby('CustomerID')}\n",
    "    for row in snapshots.itertuples(index=False):\n",
    "        cid = row.CustomerID\n",
    "        t = row.snapshot_date\n",
    "        future = has_purchase_within(by_cust.get(cid, pd.DataFrame(columns=df_all.columns)), t, t + pd.Timedelta(days=horizon_days))\n",
    "        labels.append(int(future))\n",
    "    out = snapshots.copy()\n",
    "    out['label_next_30d'] = labels\n",
    "    return out\n",
    "\n",
    "def window_stats(df_hist: pd.DataFrame) -> Dict[str, Any]:\n",
    "    # Basic RFM-like stats and behavioral mixes for the historical window\n",
    "    if df_hist.empty:\n",
    "        return {\n",
    "            'recency_days': np.nan,\n",
    "            'orders_30d': 0, 'orders_90d': 0,\n",
    "            'amount_30d': 0.0, 'amount_90d': 0.0,\n",
    "            'avg_discount_90d': np.nan,\n",
    "            **{f\"cat_{c}\": 0 for c in CAT_VALUES},\n",
    "            **{f\"pay_{p}\": 0 for p in PAY_VALUES},\n",
    "        }\n",
    "    latest = df_hist['TransactionDate'].max()\n",
    "    recency = (latest - df_hist['TransactionDate'].max()).days  # will be 0 (snapshot on last txn); include for consistency\n",
    "    # 30d/90d windows\n",
    "    w30 = df_hist['TransactionDate'] >= (latest - pd.Timedelta(days=30))\n",
    "    w90 = df_hist['TransactionDate'] >= (latest - pd.Timedelta(days=90))\n",
    "    orders_30d = int(w30.sum())\n",
    "    orders_90d = int(w90.sum())\n",
    "    amount_30d = float(df_hist.loc[w30, 'TotalAmount'].sum())\n",
    "    amount_90d = float(df_hist.loc[w90, 'TotalAmount'].sum())\n",
    "    avg_discount_90d = float(df_hist.loc[w90, 'DiscountApplied(%)'].mean()) if orders_90d>0 else np.nan\n",
    "    # Category and payment mixes in 90d\n",
    "    cat_counts = Counter(df_hist.loc[w90, 'ProductCategory'])\n",
    "    pay_counts = Counter(df_hist.loc[w90, 'PaymentMethod'])\n",
    "    feats = {\n",
    "        'recency_days': recency,\n",
    "        'orders_30d': orders_30d, 'orders_90d': orders_90d,\n",
    "        'amount_30d': amount_30d, 'amount_90d': amount_90d,\n",
    "        'avg_discount_90d': avg_discount_90d,\n",
    "    }\n",
    "    feats.update({f\"cat_{c}\": int(cat_counts.get(c, 0)) for c in CAT_VALUES})\n",
    "    feats.update({f\"pay_{p}\": int(pay_counts.get(p, 0)) for p in PAY_VALUES})\n",
    "    return feats\n",
    "\n",
    "CAT_VALUES = sorted(df['ProductCategory'].astype(str).unique().tolist())\n",
    "PAY_VALUES = sorted(df['PaymentMethod'].astype(str).unique().tolist())\n",
    "\n",
    "def build_features(df_all: pd.DataFrame, snapshots: pd.DataFrame, feature_window_days: int = 90) -> pd.DataFrame:\n",
    "    records = []\n",
    "    by_cust = {cid: grp.sort_values('TransactionDate') for cid, grp in df_all.groupby('CustomerID')}\n",
    "    for row in snapshots.itertuples(index=False):\n",
    "        cid = row.CustomerID\n",
    "        t = row.snapshot_date\n",
    "        hist = by_cust.get(cid, pd.DataFrame(columns=df_all.columns))\n",
    "        if not hist.empty:\n",
    "            hist = hist.loc[hist['TransactionDate'] <= t]\n",
    "            hist = hist.loc[hist['TransactionDate'] >= (t - pd.Timedelta(days=feature_window_days))]\n",
    "        feats = window_stats(hist)\n",
    "        feats.update({'CustomerID': cid, 'snapshot_date': t})\n",
    "        records.append(feats)\n",
    "    return pd.DataFrame.from_records(records)\n",
    "\n",
    "# Build split-specific snapshots\n",
    "train_df = filter_by_window(df, splits.train_start, splits.val_start - pd.Timedelta(days=1))\n",
    "val_df   = filter_by_window(df, splits.val_start, splits.test_start - pd.Timedelta(days=1))\n",
    "test_df  = filter_by_window(df, splits.test_start, splits.last_safe_snapshot)\n",
    "\n",
    "snapshots_train = last_snapshot_per_customer(train_df)\n",
    "snapshots_val   = last_snapshot_per_customer(val_df)\n",
    "snapshots_test  = last_snapshot_per_customer(test_df)\n",
    "logger.info(f\"Snapshots: train={len(snapshots_train)} val={len(snapshots_val)} test={len(snapshots_test)}\")\n",
    "\n",
    "# Labels\n",
    "labels_train = label_next_30d(df, snapshots_train, LABEL_HORIZON_DAYS)\n",
    "labels_val   = label_next_30d(df, snapshots_val, LABEL_HORIZON_DAYS)\n",
    "labels_test  = label_next_30d(df, snapshots_test, LABEL_HORIZON_DAYS)\n",
    "logger.info(f\"Positives (train/val/test): {labels_train['label_next_30d'].mean():.3f} / {labels_val['label_next_30d'].mean():.3f} / {labels_test['label_next_30d'].mean():.3f}\")\n",
    "\n",
    "# Features\n",
    "X_train = build_features(df, snapshots_train, FEATURE_WINDOW_DAYS)\n",
    "X_val   = build_features(df, snapshots_val, FEATURE_WINDOW_DAYS)\n",
    "X_test  = build_features(df, snapshots_test, FEATURE_WINDOW_DAYS)\n",
    "\n",
    "y_train = labels_train['label_next_30d'].values\n",
    "y_val   = labels_val['label_next_30d'].values\n",
    "y_test  = labels_test['label_next_30d'].values\n",
    "\n",
    "logger.info(f\"Feature shapes: X_train={X_train.shape}, X_val={X_val.shape}, X_test={X_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c6609a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8) Model: Logistic Regression Baseline and Evaluation\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Select numeric feature columns (exclude id/date)\n",
    "non_feature_cols = {'CustomerID','snapshot_date'}\n",
    "feature_cols = [c for c in X_train.columns if c not in non_feature_cols]\n",
    "numeric_features = feature_cols  # all are numeric counts or amounts\n",
    "\n",
    "num_pipe = Pipeline([\n",
    "    ('impute', SimpleImputer(strategy='constant', fill_value=0.0)),\n",
    "    ('scale', StandardScaler(with_mean=False)),\n",
    "])\n",
    "preprocess = ColumnTransformer([\n",
    "    ('num', num_pipe, numeric_features)\n",
    "], remainder='drop')\n",
    "\n",
    "clf = LogisticRegression(max_iter=1000, class_weight='balanced', C=1.0, solver='lbfgs')\n",
    "pipe = Pipeline([('prep', preprocess), ('clf', clf)])\n",
    "pipe.fit(X_train[numeric_features], y_train)\n",
    "\n",
    "def evaluate_split(name: str, X: pd.DataFrame, y: np.ndarray) -> Dict[str, float]:\n",
    "    proba = pipe.predict_proba(X[numeric_features])[:,1]\n",
    "    roc = roc_auc_score(y, proba)\n",
    "    pr  = average_precision_score(y, proba)\n",
    "    # Recall@Top10%\n",
    "    k = max(1, int(0.10 * len(proba)))\n",
    "    topk_idx = np.argsort(proba)[-k:]\n",
    "    recall_at_k = float(y[topk_idx].sum()) / float(y.sum()) if y.sum() > 0 else np.nan\n",
    "    logger.info(f\"{name} ROC-AUC={roc:.3f} PR-AUC={pr:.3f} Recall@Top10%={recall_at_k:.3f}\")\n",
    "    return {\"roc_auc\": float(roc), \"pr_auc\": float(pr), \"recall_at_10pct\": recall_at_k}\n",
    "\n",
    "metrics = {\n",
    "    'val': evaluate_split('VAL', X_val, y_val),\n",
    "    'test': evaluate_split('TEST', X_test, y_test),\n",
    "}\n",
    "save_json(metrics, output_dir / 'metrics_next_purchase_30d.json')\n",
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b10115",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9) Curves and Plots\n",
    "def plot_curves(X: pd.DataFrame, y: np.ndarray, tag: str):\n",
    "    proba = pipe.predict_proba(X[numeric_features])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y, proba)\n",
    "    prec, rec, _ = precision_recall_curve(y, proba)\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    axes[0].plot(fpr, tpr, label=f'ROC-AUC={roc_auc_score(y, proba):.3f}')\n",
    "    axes[0].plot([0,1],[0,1],'k--', alpha=0.5)\n",
    "    axes[0].set_title(f'ROC – Next purchase in 30 days – {tag}')\n",
    "    axes[0].set_xlabel('False Positive Rate (FPR)')\n",
    "    axes[0].set_ylabel('True Positive Rate (TPR)')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(rec, prec, label=f'PR-AUC={average_precision_score(y, proba):.3f}')\n",
    "    axes[1].set_title(f'Precision–Recall – Next 30 days – {tag}')\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].legend()\n",
    "    fig.tight_layout()\n",
    "    out_path = output_dir / f'curves_{tag.lower()}.png'\n",
    "    fig.savefig(out_path, dpi=120)\n",
    "    plt.close(fig)\n",
    "    logger.info(f\"Saved curves to {out_path}\")\n",
    "\n",
    "plot_curves(X_val, y_val, 'VAL')\n",
    "plot_curves(X_test, y_test, 'TEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d8d245b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10) Save Outputs and Artifacts\n",
    "snapshots_train.assign(split='train').to_csv(output_dir / 'snapshots_train.csv', index=False)\n",
    "snapshots_val.assign(split='val').to_csv(output_dir / 'snapshots_val.csv', index=False)\n",
    "snapshots_test.assign(split='test').to_csv(output_dir / 'snapshots_test.csv', index=False)\n",
    "save_json(metrics, output_dir / 'metrics_next_purchase_30d.json')\n",
    "print(\"Saved snapshots and metrics to outputs/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e3972e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11) Inline Unit Tests (pytest) for Helpers — Python-only runner\n",
    "import sys, tempfile, subprocess, textwrap\n",
    "import json as _json\n",
    "import pandas as pd\n",
    "\n",
    "TEST_CODE = textwrap.dedent(\n",
    "    \"\"\"\n",
    "    import json as _json\n",
    "    from pathlib import Path\n",
    "    import pandas as pd\n",
    "    import pytest\n",
    "\n",
    "    def test_json_roundtrip(tmp_path: Path):\n",
    "        obj = {\"a\": 1, \"b\": [1,2,3]}\n",
    "        p = tmp_path / \"x.json\"\n",
    "        p.write_text(_json.dumps(obj))\n",
    "        got = _json.loads(p.read_text())\n",
    "        assert got == obj\n",
    "\n",
    "    def test_pandas_read_missing(tmp_path: Path):\n",
    "        with pytest.raises(FileNotFoundError):\n",
    "            _ = pd.read_csv(tmp_path / \"missing.csv\")\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "with tempfile.NamedTemporaryFile(\"w\", suffix=\"_test.py\", delete=False) as f:\n",
    "    f.write(TEST_CODE)\n",
    "    test_path = f.name\n",
    "\n",
    "print(\"Running pytest on\", test_path)\n",
    "res = subprocess.run([sys.executable, \"-m\", \"pytest\", \"-q\", test_path], text=True, capture_output=True)\n",
    "print(res.stdout)\n",
    "if res.returncode != 0:\n",
    "    print(res.stderr, file=sys.stderr)\n",
    "    raise SystemExit(res.returncode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "801a97ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12) Profiling and Timing\n",
    "import time, pstats, cProfile\n",
    "start = time.perf_counter()\n",
    "_ = build_features(df, snapshots_val.head(50), FEATURE_WINDOW_DAYS)  # small sample for speed\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Feature build (50 snapshots) took {elapsed:.3f}s\")\n",
    "\n",
    "prof = cProfile.Profile()\n",
    "prof.enable()\n",
    "_ = build_features(df, snapshots_val.head(100), FEATURE_WINDOW_DAYS)\n",
    "prof.disable()\n",
    "ps = pstats.Stats(prof).sort_stats('cumulative')\n",
    "ps.print_stats(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d3ecdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 13) Export/Run from VS Code Terminal\n",
    "import subprocess, sys\n",
    "from datetime import datetime\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "export_py = output_dir / f\"next_purchase_30d_{timestamp}.py\"\n",
    "cmd = [sys.executable, \"-m\", \"jupyter\", \"nbconvert\", \"--to\", \"script\", str(project_root / \"notebooks\" / \"next_purchase_30d.ipynb\"), \"--output\", str(export_py)]\n",
    "print(\"Exporting to script:\", \" \".join(cmd))\n",
    "try:\n",
    "    subprocess.run(cmd, check=False)\n",
    "except Exception as e:\n",
    "    print(\"Export failed (ok to ignore if nbconvert not installed):\", e)\n",
    "\n",
    "print(\"You can run the exported script or prefer running this notebook interactively.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735fa9f4",
   "metadata": {},
   "source": [
    "## 14) Refine feature functions (fix recency and windows)\n",
    "This section refines how recency and rolling windows are computed to ensure no label leakage and clearer feature semantics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa5abf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14) Refine feature functions (fix recency and windows)\n",
    "def window_stats(snapshot_date: pd.Timestamp, df_hist: pd.DataFrame) -> Dict[str, Any]:\n",
    "    if df_hist.empty:\n",
    "        return {\n",
    "            'recency_days': np.nan,\n",
    "            'orders_30d': 0, 'orders_90d': 0,\n",
    "            'amount_30d': 0.0, 'amount_90d': 0.0,\n",
    "            'avg_discount_90d': np.nan,\n",
    "            **{f\"cat_{c}\": 0 for c in CAT_VALUES},\n",
    "            **{f\"pay_{p}\": 0 for p in PAY_VALUES},\n",
    "        }\n",
    "    prev_dates = df_hist.loc[df_hist['TransactionDate'] < snapshot_date, 'TransactionDate']\n",
    "    last_prev = prev_dates.max() if not prev_dates.empty else pd.NaT\n",
    "    recency = (snapshot_date - last_prev).days if pd.notna(last_prev) else np.nan\n",
    "    w30 = (df_hist['TransactionDate'] > (snapshot_date - pd.Timedelta(days=30))) & (df_hist['TransactionDate'] <= snapshot_date)\n",
    "    w90 = (df_hist['TransactionDate'] > (snapshot_date - pd.Timedelta(days=90))) & (df_hist['TransactionDate'] <= snapshot_date)\n",
    "    orders_30d = int(w30.sum())\n",
    "    orders_90d = int(w90.sum())\n",
    "    amount_30d = float(df_hist.loc[w30, 'TotalAmount'].sum())\n",
    "    amount_90d = float(df_hist.loc[w90, 'TotalAmount'].sum())\n",
    "    avg_discount_90d = float(df_hist.loc[w90, 'DiscountApplied(%)'].mean()) if orders_90d>0 else np.nan\n",
    "    cat_counts = Counter(df_hist.loc[w90, 'ProductCategory'])\n",
    "    pay_counts = Counter(df_hist.loc[w90, 'PaymentMethod'])\n",
    "    feats = {\n",
    "        'recency_days': recency,\n",
    "        'orders_30d': orders_30d, 'orders_90d': orders_90d,\n",
    "        'amount_30d': amount_30d, 'amount_90d': amount_90d,\n",
    "        'avg_discount_90d': avg_discount_90d,\n",
    "    }\n",
    "    feats.update({f\"cat_{c}\": int(cat_counts.get(c, 0)) for c in CAT_VALUES})\n",
    "    feats.update({f\"pay_{p}\": int(pay_counts.get(p, 0)) for p in PAY_VALUES})\n",
    "    return feats\n",
    "\n",
    "def build_features(df_all: pd.DataFrame, snapshots: pd.DataFrame, feature_window_days: int = 90) -> pd.DataFrame:\n",
    "    records = []\n",
    "    by_cust = {cid: grp.sort_values('TransactionDate') for cid, grp in df_all.groupby('CustomerID')}\n",
    "    for row in snapshots.itertuples(index=False):\n",
    "        cid = row.CustomerID\n",
    "        t = row.snapshot_date\n",
    "        hist = by_cust.get(cid, pd.DataFrame(columns=df_all.columns))\n",
    "        if not hist.empty:\n",
    "            hist = hist.loc[(hist['TransactionDate'] <= t) & (hist['TransactionDate'] >= (t - pd.Timedelta(days=feature_window_days)))]\n",
    "        feats = window_stats(t, hist)\n",
    "        feats.update({'CustomerID': cid, 'snapshot_date': t})\n",
    "        records.append(feats)\n",
    "    return pd.DataFrame.from_records(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ea3c4ba",
   "metadata": {},
   "source": [
    "## 15) Transform API\n",
    "Wraps the label and feature building into a reusable transform(transactions, params) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c7a371",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15) Transform API\n",
    "def transform(transactions: pd.DataFrame, params: Optional[Dict[str, Any]] = None) -> Tuple[pd.DataFrame, pd.Series]:\n",
    "    \"\"\"\n",
    "    Build features and labels for next-30d prediction using one snapshot per customer per split\n",
    "    (the customer's last transaction inside the split window).\n",
    "\n",
    "    transactions: DataFrame with required columns:\n",
    "      ['CustomerID','ProductID','Quantity','Price','TransactionDate',\n",
    "       'PaymentMethod','StoreLocation','ProductCategory','DiscountApplied(%)','TotalAmount']\n",
    "    params: optional dict with keys:\n",
    "      - label_horizon_days (int)\n",
    "      - feature_window_days (int)\n",
    "      - split ('train'|'val'|'test') to select snapshots\n",
    "    Returns: (X, y) where X are features and y is the binary label Series.\n",
    "    \"\"\"\n",
    "    p = {\n",
    "        'label_horizon_days': LABEL_HORIZON_DAYS,\n",
    "        'feature_window_days': FEATURE_WINDOW_DAYS,\n",
    "        'split': 'train',\n",
    "    }\n",
    "    if params: p.update(params)\n",
    "    splits_local = compute_split_windows(transactions, p['label_horizon_days'])\n",
    "    if p['split'] == 'train':\n",
    "        dwin = filter_by_window(transactions, splits_local.train_start, splits_local.val_start - pd.Timedelta(days=1))\n",
    "    elif p['split'] == 'val':\n",
    "        dwin = filter_by_window(transactions, splits_local.val_start, splits_local.test_start - pd.Timedelta(days=1))\n",
    "    else:\n",
    "        dwin = filter_by_window(transactions, splits_local.test_start, splits_local.last_safe_snapshot)\n",
    "    snapshots = last_snapshot_per_customer(dwin)\n",
    "    labels = label_next_30d(transactions, snapshots, p['label_horizon_days'])['label_next_30d']\n",
    "    X = build_features(transactions, snapshots, p['feature_window_days'])\n",
    "    return X, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5899d43a",
   "metadata": {},
   "source": [
    "## 16) Plot helper adjustments\n",
    "Clarifies the plot_curves helper to use the numeric_features list consistently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a5873c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 16) Plot helper adjustments\n",
    "def plot_curves(X: pd.DataFrame, y: np.ndarray, tag: str):\n",
    "    proba = pipe.predict_proba(X[numeric_features])[:,1]\n",
    "    fpr, tpr, _ = roc_curve(y, proba)\n",
    "    prec, rec, _ = precision_recall_curve(y, proba)\n",
    "    fig, axes = plt.subplots(1,2, figsize=(12,5))\n",
    "    axes[0].plot(fpr, tpr, label=f'ROC-AUC={roc_auc_score(y, proba):.3f}')\n",
    "    axes[0].plot([0,1],[0,1],'k--', alpha=0.5)\n",
    "    axes[0].set_title(f'ROC – Next purchase in 30 days – {tag}')\n",
    "    axes[0].set_xlabel('False Positive Rate (FPR)')\n",
    "    axes[0].set_ylabel('True Positive Rate (TPR)')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(rec, prec, label=f'PR-AUC={average_precision_score(y, proba):.3f}')\n",
    "    axes[1].set_title(f'Precision–Recall – Next 30 days – {tag}')\n",
    "    axes[1].set_xlabel('Recall')\n",
    "    axes[1].set_ylabel('Precision')\n",
    "    axes[1].legend()\n",
    "    fig.tight_layout()\n",
    "    out_path = output_dir / f'curves_{tag.lower()}.png'\n",
    "    fig.savefig(out_path, dpi=120)\n",
    "    plt.close(fig)\n",
    "    logger.info(f\"Saved curves to {out_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbcd84e",
   "metadata": {},
   "source": [
    "## 17) Rebuild features and retrain (refined)\n",
    "Recomputes features with the refined logic and retrains the model; metrics are compared to the initial baseline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4d5c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 17) Rebuild features with refined logic and retrain\n",
    "# Recompute features after refined window functions\n",
    "X_train = build_features(df, snapshots_train, FEATURE_WINDOW_DAYS)\n",
    "X_val   = build_features(df, snapshots_val, FEATURE_WINDOW_DAYS)\n",
    "X_test  = build_features(df, snapshots_test, FEATURE_WINDOW_DAYS)\n",
    "logger.info(f\"[REFINED] Feature shapes: X_train={X_train.shape}, X_val={X_val.shape}, X_test={X_test.shape}\")\n",
    "\n",
    "# Retrain baseline\n",
    "pipe.fit(X_train[numeric_features], y_train)\n",
    "metrics_refined = {\n",
    "    'val': evaluate_split('VAL-REFINED', X_val, y_val),\n",
    "    'test': evaluate_split('TEST-REFINED', X_test, y_test),\n",
    "}\n",
    "save_json(metrics_refined, output_dir / 'metrics_next_purchase_30d_refined.json')\n",
    "metrics_refined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4641caf5",
   "metadata": {},
   "source": [
    "## 18) Export ranked predictions (who is most likely to buy in 30 days)\n",
    "Creates per-customer ranked prediction lists for Validation and Test splits with scores, true labels, and a Top-10% flag. Files are saved to the outputs/ folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c39ac9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 18) Export ranked predictions for VAL and TEST\n",
    "import numpy as _np\n",
    "import pandas as _pd\n",
    "\n",
    "def export_ranked_predictions(split_name: str, X: pd.DataFrame, snapshots: pd.DataFrame, labels: pd.DataFrame, top_pct: float = 0.10):\n",
    "    proba = pipe.predict_proba(X[numeric_features])[:,1]\n",
    "    df_pred = _pd.DataFrame({\n",
    "        'CustomerID': X['CustomerID'].values if 'CustomerID' in X.columns else snapshots['CustomerID'].values,\n",
    "        'snapshot_date': X['snapshot_date'].values if 'snapshot_date' in X.columns else snapshots['snapshot_date'].values,\n",
    "        'predicted_probability_30d': proba,\n",
    "        'label_next_30d': labels['label_next_30d'].values,\n",
    "    })\n",
    "    df_pred = df_pred.sort_values('predicted_probability_30d', ascending=False).reset_index(drop=True)\n",
    "    k = max(1, int(len(df_pred) * top_pct))\n",
    "    cutoff = df_pred.iloc[k-1]['predicted_probability_30d'] if len(df_pred) >= k else df_pred['predicted_probability_30d'].min()\n",
    "    df_pred['is_top_10_percent'] = df_pred['predicted_probability_30d'] >= cutoff\n",
    "    out_path = output_dir / f'predictions_{split_name.lower()}.csv'\n",
    "    df_pred.to_csv(out_path, index=False)\n",
    "    print(f\"Saved {split_name} predictions to {out_path} (top {int(top_pct*100)}% flagged)\")\n",
    "    return df_pred.head(20)\n",
    "\n",
    "display(export_ranked_predictions('VAL', X_val, snapshots_val, labels_val))\n",
    "display(export_ranked_predictions('TEST', X_test, snapshots_test, labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481bc24",
   "metadata": {},
   "source": [
    "## 19) What gets saved where (outputs map)\n",
    "- metrics_next_purchase_30d.json: created in Section 8 (baseline metrics).\n",
    "- metrics_next_purchase_30d_refined.json: created in Section 17 (refined features retrain).\n",
    "- curves_val.png and curves_test.png: created in Sections 9/16 (ROC and PR plots for Validation/Test).\n",
    "- snapshots_train.csv, snapshots_val.csv, snapshots_test.csv: created in Section 10 (one snapshot per customer per split).\n",
    "- predictions_val.csv, predictions_test.csv: created in Section 18 (ranked customers with predicted_probability_30d and is_top_10_percent).\n",
    "\n",
    "Tip: Open these files in `outputs/` after running the corresponding section to see the latest artifacts.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
